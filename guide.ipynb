{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Sign Detection and Classification\n",
    "This work focuses on the detection of color and shapes regarding traffic signs under many different circumstances and combinations of illumination, angle, and contrast.\n",
    "\n",
    "The first step is to import all the necessaries libraries that will be used in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import imutils\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables\n",
    "- `FILENAME`: Define the road or parse all roads inside the `data` folder. `ALL` to show all images.\n",
    "- `TYPE`: `WINDOW` to open an window with the image, `FILE` to create a file with the file.  \n",
    "- `Debug`: Used for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"ALL\"\n",
    "OUTPUT = \"WINDOW\"\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data\n",
    "\n",
    "Class used to store information about the image.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, filename) -> None:\n",
    "        self.filename = filename\n",
    "        self.image = cv2.imread(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_stop(img1, img2):\n",
    "\n",
    "    MIN_MATCH_COUNT = 5\n",
    "\n",
    "    img1 = cv2.imread(img1,cv2.IMREAD_GRAYSCALE) # queryImage\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    img1 = cv2.GaussianBlur(img1, (5, 5), 0)\n",
    "    img2 = cv2.GaussianBlur(img2, (5, 5), 0)\n",
    "\n",
    "    # Initiate SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks = 50)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    if len(good)>MIN_MATCH_COUNT:\n",
    "        src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "        matchesMask = mask.ravel().tolist()\n",
    "\n",
    "        h,w = img1.shape\n",
    "        pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "        dst = cv2.perspectiveTransform(pts,M)\n",
    "\n",
    "        img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "\n",
    "    else:\n",
    "        #print(\"Not enough matches are found - \" + str(len(good)) + \"/\" + str(MIN_MATCH_COUNT))\n",
    "        matchesMask = None\n",
    "        return\n",
    "\n",
    "    print(good)\n",
    "    draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                       singlePointColor = None,\n",
    "                       matchesMask = matchesMask, # draw only inliers\n",
    "                       flags = 2)\n",
    "    \n",
    "    list_kp2 = [kp2[mat.trainIdx].pt for mat in good]\n",
    "    x,y = sum(i for i, j in list_kp2), sum(j for i, j in list_kp2)\n",
    "    x_mean = int(x/len(good))\n",
    "    y_mean = int(y/len(good))\n",
    "    \n",
    "    cv2.putText(img2, \"sign\", (x_mean,y_mean), cv2.FONT_HERSHEY_SIMPLEX,0.5, (255, 255, 255), 2)\n",
    "    \n",
    "    img3 = cv2.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)\n",
    "\n",
    "    cv2.imshow(\"Image\", img2)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape detection class\n",
    "\n",
    "This class is used to detect the Shape present in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeDetector:\n",
    "    def __init__(self, red, blue, image) -> None:\n",
    "        self.red = red\n",
    "        self.blue = blue\n",
    "        self.image = image\n",
    "\n",
    "    def _shape_name(self, contour, colour):\n",
    "        peri = cv2.arcLength(contour, True)\n",
    "        factor = 0.02 if colour == \"blue\" else 0.01\n",
    "        approx = cv2.approxPolyDP(contour, factor * peri, True) # Estava 0.01, verificar tudo de novo a ver se n√£o estraga...            \n",
    "\n",
    "        if len(approx) == 3:\n",
    "            return \"triangle\"\n",
    "        elif len(approx) == 4:\n",
    "            (_, _, width, height) = cv2.boundingRect(approx)\n",
    "            aspect_ratio = width / float(height)\n",
    "\n",
    "            # a square will have an aspect ratio that is approximately equal to one, otherwise, the shape is a rectangle\n",
    "            return \"square\" if 0.90 <= aspect_ratio <= 1.10 else \"rectangle\"\n",
    "        elif 6 <= len(approx) <= 7:\n",
    "            # if the shape is a triangle, it will have can have 6 or 7 vertices (due to the corner curves)\n",
    "            distances = []\n",
    "            for x,y in zip(approx, approx[1:]):\n",
    "                d = math.sqrt((x[0][1]-y[0][1])*(x[0][1]-y[0][1]) + (x[0][0]-y[0][0])*(x[0][0]-y[0][0]))\n",
    "                distances.append(d)\n",
    "\n",
    "            distances.sort()\n",
    "            if distances[len(approx) - 5] < 1/4 * distances[len(approx) - 4] or distances[len(approx) - 4] < 1/4 * distances[len(approx) - 3]:\n",
    "                return  \"triangle\"\n",
    "        elif len(approx) == 8 and colour == \"red\":\n",
    "            return  \"stop\"\n",
    "\n",
    "        print(len(approx))\n",
    "        return \"unidentified\"\n",
    "\n",
    "    def _shape_countours(self, image, colour):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n",
    "        blurred = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "        blurred = cv2.threshold(blurred, 20, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "        # cv2.imshow('THRESGOL', blurred)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "\n",
    "        final_contours = []\n",
    "\n",
    "        # find contours in the thresholded image and initialize the shape detector\n",
    "        countours = cv2.findContours(blurred.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        countours = imutils.grab_contours(countours)\n",
    "\n",
    "        # Draw circles\n",
    "        # TODO: Verify better values\n",
    "         # param1 estava a 100, mudamos por causa do road53.png, mudamos param2 de 0.8 para 0.5\n",
    "        circles = cv2.HoughCircles(blurred, cv2.HOUGH_GRADIENT_ALT, 1.5, 30, param1=250, param2=0.75, minRadius=1)\n",
    "\n",
    "        if circles is None: circles = [[]]\n",
    "        circles = np.uint16(np.around(circles))\n",
    "\n",
    "        processed_centers = {}\n",
    "        for i in circles[0,:]:\n",
    "            if (i[0],i[1]) in processed_centers.keys():\n",
    "                if processed_centers[(i[0],i[1])] < i[2]:\n",
    "                    processed_centers[(i[0],i[1])] = i[2]\n",
    "            else:\n",
    "                processed_centers[(i[0], i[1])] = i[2]\n",
    "\n",
    "        for c in countours:\n",
    "            # TODO: Area?\n",
    "            # Area of the Image\n",
    "            # AREA = img.shape[0]*img.shape[1]/20\n",
    "            AREA = 1000\n",
    "            if cv2.contourArea(c) < AREA:\n",
    "                continue\n",
    "            \n",
    "            # Shape of the Image\n",
    "            shape = self._shape_name(c, colour)\n",
    "            if shape == \"unidentified\":\n",
    "                continue\n",
    "\n",
    "            # Compute the center of the contour, then detect the name of the shape using only the contour\n",
    "            M = cv2.moments(c)\n",
    "            cX = int((M[\"m10\"] / (M[\"m00\"] + 1e-7))) #* ratio)\n",
    "            cY = int((M[\"m01\"] / (M[\"m00\"] + 1e-7))) #* ratio)\n",
    "            \n",
    "            # TODO: Ratio?\n",
    "            # multiply the contour (x, y)-coordinates by the resize ratio, then draw the contours and the name of the shape on the image\n",
    "            c = c.astype(\"float\")\n",
    "            # c *= ratio\n",
    "            c = c.astype(\"int\")\n",
    "            \n",
    "            # TODO: If the building is red still detects a stop sign - road78.png\n",
    "            # if not contains_circle:\n",
    "            if not (shape == \"stop\" and colour == \"blue\"): \n",
    "                final_contours.append((shape, c, colour))\n",
    "\n",
    "                # Draw and write shape name\n",
    "                cv2.drawContours(image, [c], -1, (0, 255, 0), 2)\n",
    "                classification = f\"{colour} {shape}\" if shape != \"stop\" else \"stop sign\"\n",
    "                cv2.putText(image, classification, (cX - 35, cY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "        for (i[0],i[1]) in list(processed_centers.keys()):\n",
    "            circle = {'center': (i[0],i[1]), 'radius': (processed_centers[i[0],i[1]])}\n",
    "            final_contours.append((\"circle\", circle, colour))\n",
    "\n",
    "            # Draw the circle, center and write the shape name\n",
    "            cv2.circle(image,(i[0],i[1]),processed_centers[i[0],i[1]],(0,255,0),2)\n",
    "            cv2.circle(image,(i[0],i[1]),2,(0,0,255),3)\n",
    "            classification = f\"{colour} circle\"\n",
    "            cv2.putText(image, classification, (i[0] - 35, i[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "        return final_contours\n",
    "\n",
    "    def find_shape(self):\n",
    "        red_contours = self._shape_countours(self.red, \"red\")\n",
    "        blue_contours = self._shape_countours(self.blue, \"blue\")\n",
    "\n",
    "        return red_contours + blue_contours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Detection Class\n",
    "\n",
    "Used to detect the color of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorDetector:\n",
    "    def __init__(self, image) -> None:\n",
    "        self.image = image\n",
    "\n",
    "    def find_color(self):\n",
    "        hsv = cv2.cvtColor(self.image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        h,s,v = cv2.split(hsv)\n",
    "        hue = hsv[:, :, 0].mean()\n",
    "        saturation = hsv[:, :, 1].mean()\n",
    "\n",
    "        # CLAHE: used to increase contrast\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        swithCLAHE = clahe.apply(s)\n",
    "        vwithCLAHE = clahe.apply(v)\n",
    "\n",
    "        hsv = cv2.merge([h, swithCLAHE, vwithCLAHE])\n",
    "\n",
    "        # Generate lower mask (0-10) and upper mask (170-180) of red\n",
    "        if saturation < 61:\n",
    "            if hue < 70:\n",
    "                red_mask1 = cv2.inRange(hsv, (0,90,50), (20,255,255)) # deteta tudo no road56.png\n",
    "                red_mask2 = cv2.inRange(hsv, (160,90,50), (180,255,255)) # deteta tudo no road56.png\n",
    "            else:\n",
    "                red_mask1 = cv2.inRange(hsv, (0,90,50), (10,255,255)) # deteta tudo no road56.png\n",
    "                red_mask2 = cv2.inRange(hsv, (170,90,50), (180,255,255)) # deteta tudo no road56.png\n",
    "        else:\n",
    "            if hue < 70:\n",
    "                red_mask1 = cv2.inRange(hsv, (0,30,30), (20,255,255)) # deteta tudo no road66.png\n",
    "                red_mask2 = cv2.inRange(hsv, (160,30,30), (180,255,255)) # deteta tudo no road66.png\n",
    "            else:\n",
    "                red_mask1 = cv2.inRange(hsv, (0,30,30), (10,255,255)) # deteta tudo no road66.png\n",
    "                red_mask2 = cv2.inRange(hsv, (170,30,30), (180,255,255)) # deteta tudo no road66.png\n",
    "\n",
    "            # red_mask1 = cv2.inRange(hsv, (0,65,30), (20,255,255)) # deteta tudo no road57.png\n",
    "            # red_mask2 = cv2.inRange(hsv, (160,65,30), (180,255,255)) # deteta tudo no road57.png\n",
    "            # red_mask1 = cv2.inRange(hsv, (0,30,30), (20,255,255)) # deteta tudo no road66.png\n",
    "            # red_mask2 = cv2.inRange(hsv, (160,30,30), (180,255,255)) # deteta tudo no road66.png\n",
    "\n",
    "        # Merge the mask and crop the red regions\n",
    "        red_mask = cv2.bitwise_or(red_mask1, red_mask2)\n",
    "\n",
    "        # Generate mask (100-140) of blue\n",
    "        if saturation < 40:\n",
    "            blue_mask = cv2.inRange(hsv, (100,130,50), (140,255,255))\n",
    "        else:\n",
    "            blue_mask = cv2.inRange(hsv, (100,220,50), (140,255,255))\n",
    "\n",
    "        print(\"saturacao\" + str(saturation))\n",
    "        print(\"hue\" + str(hue))\n",
    "        red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
    "        # red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_DILATE, np.ones((3,3),np.uint8))\n",
    "\n",
    "        blue_mask = cv2.morphologyEx(blue_mask, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
    "        blue_mask = cv2.morphologyEx(blue_mask, cv2.MORPH_DILATE, np.ones((3,3),np.uint8))\n",
    "\n",
    "        red = cv2.bitwise_and(self.image, self.image, mask=red_mask)\n",
    "        blue = cv2.bitwise_and(self.image, self.image, mask=blue_mask)\n",
    "\n",
    "        red_hsv = cv2.cvtColor(red, cv2.COLOR_BGR2HSV)\n",
    "        average_hsv_1 = cv2.mean(red_hsv, red_mask1)[:3]\n",
    "        average_hsv_2 = cv2.mean(red_hsv, red_mask2)[:3] \n",
    "        average_hsv_red = cv2.mean(red_hsv,  red_mask)[:3]\n",
    "\n",
    "        blue_hsv = cv2.cvtColor(blue, cv2.COLOR_BGR2HSV)\n",
    "        average_hsv_blue = cv2.mean(blue_hsv,  blue_mask)[:3]\n",
    "\n",
    "        min_value_saturation_red = red_hsv[np.where(red_hsv[:,:,1]>0)][:,1].min() if red_hsv.any() else 100\n",
    "        min_value_saturation_blue = blue_hsv[np.where(blue_hsv[:,:,1]>0)][:,1].min() if blue_hsv.any() else 100\n",
    "\n",
    "        red_threshold = (average_hsv_red[1] + min_value_saturation_red) / 2\n",
    "        blue_threshold = (average_hsv_blue[1] + min_value_saturation_blue) / 2\n",
    "\n",
    "        print(\"thresohold azul\" + str(blue_threshold))\n",
    "        # blue_threshold = 200\n",
    "\n",
    "        max_red1 = 10 if average_hsv_1[0] <= 15 else 20\n",
    "        min_red2 = 170 if average_hsv_2[0] > 175 else 160 # TODO: devia ser average_hsv_2, mudar para verificar se n√£o estraga\n",
    "\n",
    "        red_mask1 = cv2.inRange(hsv, (0, red_threshold, 50), (max_red1, 255, 255))\n",
    "        red_mask2 = cv2.inRange(hsv, (min_red2, red_threshold, 50), (180, 255, 255))\n",
    "        \n",
    "        blue_mask = cv2.inRange(hsv, (90, blue_threshold, 50), (130, 255, 255))\n",
    "\n",
    "        # Merge the mask and crop the red regions\n",
    "        red_mask = cv2.bitwise_or(red_mask1, red_mask2)\n",
    "        red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
    "\n",
    "        blue_mask = cv2.morphologyEx(blue_mask, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
    "        blue_mask = cv2.morphologyEx(blue_mask, cv2.MORPH_DILATE, np.ones((3,3),np.uint8))\n",
    "\n",
    "        red = cv2.bitwise_and(self.image, self.image, mask=red_mask)\n",
    "        blue = cv2.bitwise_and(self.image, self.image, mask=blue_mask)\n",
    "\n",
    "        # Show red tracing\n",
    "        if DEBUG:\n",
    "            cv2.imshow('Red Color', red)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        # Show blue tracing\n",
    "        if DEBUG:\n",
    "            cv2.imshow('Blue Color', blue)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        mask = cv2.bitwise_or(red, blue)\n",
    "        result = cv2.bitwise_and(self.image, mask)\n",
    "\n",
    "        # Show blue and red tracing\n",
    "        if DEBUG:\n",
    "            cv2.imshow('Red Color Detection', red)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "        return (\"gray\", red, blue, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Evaluation\n",
    "\n",
    "Function to evaluate the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image(image_data):\n",
    "    color_detector = ColorDetector(image_data.image)\n",
    "    image_color, red_result, blue_result, color_result = color_detector.find_color()\n",
    "\n",
    "    shape_detector = ShapeDetector(red_result, blue_result, color_result)\n",
    "    contours = shape_detector.find_shape()\n",
    "\n",
    "    processed_contours = []\n",
    "    for t, c, colour in contours:\n",
    "        if t == \"circle\":\n",
    "            center = (c['center'][0], c['center'][1])\n",
    "        else:\n",
    "            M = cv2.moments(c)\n",
    "            cX = int((M[\"m10\"] / (M[\"m00\"] + 1e-7))) #* ratio)\n",
    "            cY = int((M[\"m01\"] / (M[\"m00\"] + 1e-7))) #* ratio)\n",
    "            center = (cX, cY)\n",
    "\n",
    "        contour_radius = c['radius'] if t == \"circle\" else cv2.minEnclosingCircle(c)[1]\n",
    "        if not processed_contours:\n",
    "            processed_contours.append((t, c, colour))\n",
    "            continue\n",
    "\n",
    "        invalid_contours = []\n",
    "        append_contour = False\n",
    "        processed_verified = 0\n",
    "        \n",
    "        #print(f'LEN PROCESSED:::: {len(processed_contours)}')\n",
    "\n",
    "        for (tp, cp, colourp) in processed_contours:\n",
    "            #print('PROCESSED')\n",
    "            #print(f'TYPE {tp}, colour {colourp}')\n",
    "\n",
    "            circle_inside = False\n",
    "            dist = 0\n",
    "            processed_contour_radius = cp['radius'] if tp == \"circle\" else cv2.minEnclosingCircle(cp)[1]\n",
    "\n",
    "            if tp == \"circle\":\n",
    "                #print(f'processed_contour_radius: {processed_contour_radius}')\n",
    "                #print(f'center do processed: {int(cp[\"center\"][0]), int(cp[\"center\"][1])}')\n",
    "                #print(f'center do contorno: {int(center[0]), int(center[1])}')\n",
    "                #print(f'quadrado 1: {(int(cp[\"center\"][0]) - int(center[0]))**2}')\n",
    "                #print(f'quadrado 2: {(int(cp[\"center\"][1]) - int(center[1]))**2}')\n",
    "                #print(f'quadrado 3: {int(processed_contour_radius)**2}')\n",
    "                if (int(cp['center'][0]) - int(center[0]))**2 + (int(cp['center'][1]) - int(center[1]))**2 < processed_contour_radius**2: # check if center of circle is inside the contour\n",
    "                    circle_inside = True\n",
    "            else:\n",
    "                dist = cv2.pointPolygonTest(cp, center, True)\n",
    "            if dist > 0 or circle_inside: # center of the contour is inside a processed contour\n",
    "                #print(f'PRIMEIRA CONDI√áAO {(contour_radius >= 2/3 * processed_contour_radius and ((colourp == \"red\" and t == \"stop\") or (colour == \"red\" and tp != \"stop\")))}')\n",
    "                #print(f'SEGUNDA CONDI√áAO {(contour_radius >= processed_contour_radius and colour != \"blue\")}')\n",
    "                if (contour_radius >= 2/3 * processed_contour_radius and colour == \"red\" and not (t == \"stop\" or tp == \"stop\")) or (contour_radius >= processed_contour_radius and colour != \"blue\"): # the radius of the contour is bigger than the radius of the processed contour\n",
    "                    invalid_contours.append((tp, cp, colourp))\n",
    "                    append_contour = True\n",
    "            else:\n",
    "                processed_verified += 1\n",
    "\n",
    "\n",
    "        for i in invalid_contours:\n",
    "            processed_contours.remove(i)\n",
    "        if append_contour or (processed_verified == len(processed_contours)):\n",
    "            processed_contours.append((t, c, colour))\n",
    "\n",
    "    signs = []\n",
    "    stop_count = 0\n",
    "    for t, c, colour in processed_contours:\n",
    "        signs.append(t)\n",
    "        if t == \"circle\":\n",
    "            cv2.circle(image_data.image,(c['center']),c['radius'],(0,255,0),2) # draw the outer circle\n",
    "            cv2.circle(image_data.image,(c['center']),2,(0,0,255),3) # draw the center of the circle\n",
    "            cv2.putText(image_data.image, f\"{colour} circle\", (c['center'][0] - 35, c['center'][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2) # write the shape\n",
    "        else:\n",
    "            if t == \"stop\": stop_count += 1\n",
    "            M = cv2.moments(c)\n",
    "            cX = int((M[\"m10\"] / (M[\"m00\"] + 1e-7))) #* ratio)\n",
    "            cY = int((M[\"m01\"] / (M[\"m00\"] + 1e-7))) #* ratio)\n",
    "            cv2.drawContours(image_data.image, [c], -1, (0, 255, 0), 2)\n",
    "            classification = f\"{colour} {t}\" if t != \"stop\" else \"stop sign\"\n",
    "            cv2.putText(image_data.image, classification, (cX - 35, cY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "    \n",
    "    if stop_count == 0:\n",
    "        stop_sign = os.path.join(\"./data\", \"stop.png\")\n",
    "        matching_stop(stop_sign, image_data.image)\n",
    "\n",
    "    return signs, image_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will run our code, processing the request images.\n",
    "\n",
    "- Press any key to see the next image, if any\n",
    "- Press the key `Esc` to exit the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saturacao60.80605172413793\n",
      "hue28.472922413793103\n",
      "thresohold azul50.0\n",
      "------\n",
      "['stop', 'circle', 'circle']\n",
      "['stop']\n",
      "print53\n",
      "saturacao59.31404166666667\n",
      "hue74.20561666666667\n",
      "thresohold azul50.0\n",
      "18\n",
      "7\n",
      "------\n",
      "['stop', 'circle']\n",
      "['stop']\n",
      "print54\n",
      "saturacao65.93465355805243\n",
      "hue64.5320786516854\n",
      "thresohold azul50.0\n",
      "12\n",
      "8\n",
      "[<DMatch 0000020FD3F6F610>, <DMatch 0000020FD3F6F4B0>, <DMatch 0000020FD3F6EC90>, <DMatch 0000020FD3F6F230>, <DMatch 0000020FD3F6F2F0>, <DMatch 0000020FD3F6F2D0>, <DMatch 0000020FD3F6F290>, <DMatch 0000020FD3F6F050>, <DMatch 0000020FD3F6EEF0>]\n",
      "------\n",
      "['circle']\n",
      "['stop']\n",
      "print55\n",
      "saturacao48.391725\n",
      "hue22.82315\n",
      "thresohold azul50.0\n",
      "------\n",
      "['stop', 'circle', 'circle']\n",
      "['stop']\n",
      "print56\n",
      "saturacao118.353775\n",
      "hue42.57380833333333\n",
      "thresohold azul50.0\n",
      "6\n",
      "8\n",
      "8\n",
      "------\n",
      "['stop']\n",
      "['stop']\n",
      "print57\n",
      "saturacao107.33051495016612\n",
      "hue52.91627906976744\n",
      "thresohold azul50.0\n",
      "------\n",
      "['stop']\n",
      "['stop']\n",
      "print58\n",
      "saturacao138.8392441860465\n",
      "hue39.620531561461796\n",
      "thresohold azul50.0\n",
      "------\n",
      "['stop']\n",
      "['stop']\n",
      "print59\n",
      "saturacao123.96198125\n",
      "hue31.80425\n",
      "thresohold azul50.0\n",
      "------\n",
      "['stop']\n",
      "['stop']\n",
      "print60\n",
      "saturacao66.52696013289037\n",
      "hue122.69143687707641\n",
      "thresohold azul50.0\n",
      "6\n",
      "9\n",
      "9\n",
      "9\n",
      "------\n",
      "['stop', 'circle']\n",
      "['stop']\n",
      "print61\n",
      "saturacao51.512022471910115\n",
      "hue76.58051498127341\n",
      "thresohold azul50.0\n",
      "7\n",
      "------\n",
      "['stop']\n",
      "['stop']\n",
      "print62\n",
      "saturacao88.3437707641196\n",
      "hue41.856029900332224\n",
      "thresohold azul50.0\n",
      "16\n",
      "9\n",
      "[<DMatch 0000020FD3F6F5D0>, <DMatch 0000020FD3F6F5B0>, <DMatch 0000020FD3F6FAD0>, <DMatch 0000020FD3F6F1B0>, <DMatch 0000020FD3F6F270>, <DMatch 0000020FD3F6F950>]\n",
      "------\n",
      "[]\n",
      "['stop']\n",
      "print63\n",
      "saturacao170.05369601328903\n",
      "hue134.7611877076412\n",
      "thresohold azul50.0\n",
      "------\n",
      "['stop']\n",
      "['stop']\n",
      "print64\n",
      "saturacao36.24289179104478\n",
      "hue47.09916044776119\n",
      "thresohold azul50.0\n",
      "15\n",
      "16\n",
      "12\n",
      "11\n",
      "------\n",
      "['stop', 'circle', 'circle']\n",
      "['stop']\n",
      "print65\n",
      "saturacao62.78265780730897\n",
      "hue94.3297342192691\n",
      "thresohold azul50.0\n",
      "10\n",
      "7\n",
      "8\n",
      "13\n",
      "11\n",
      "------\n",
      "['circle', 'circle', 'circle', 'circle']\n",
      "['stop']\n",
      "print66\n",
      "saturacao66.69875\n",
      "hue38.83299074074074\n",
      "thresohold azul50.0\n",
      "14\n",
      "10\n",
      "6\n",
      "7\n",
      "------\n",
      "['stop']\n",
      "['stop']\n",
      "print67\n",
      "saturacao91.22105833333333\n",
      "hue72.580675\n",
      "thresohold azul50.0\n",
      "17\n",
      "14\n",
      "14\n",
      "9\n",
      "------\n",
      "['stop']\n",
      "['stop']\n",
      "print68\n",
      "saturacao79.18509966777408\n",
      "hue112.70538205980067\n",
      "thresohold azul50.0\n",
      "9\n",
      "------\n",
      "['stop']\n",
      "['stop']\n",
      "print69\n",
      "saturacao65.4080315614618\n",
      "hue26.241951827242524\n",
      "thresohold azul50.0\n",
      "20\n",
      "9\n",
      "6\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\matmul.dispatch.cpp:550: error: (-215:Assertion failed) scn + 1 == m.cols in function 'cv::perspectiveTransform'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\diogo\\Documents\\cgra\\feup-vcom-traffic-sign-detection\\guide.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000013?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m FILENAME \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mALL\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000013?line=26'>27</a>\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m53\u001b[39m, \u001b[39m876\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000013?line=27'>28</a>\u001b[0m                 process_image(i)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000013?line=28'>29</a>\u001b[0m                 \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mprint\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000013?line=29'>30</a>\u001b[0m                 k \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m0\u001b[39m)   \n",
      "\u001b[1;32mc:\\Users\\diogo\\Documents\\cgra\\feup-vcom-traffic-sign-detection\\guide.ipynb Cell 15'\u001b[0m in \u001b[0;36mprocess_image\u001b[1;34m(road_number)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000013?line=1'>2</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mroad\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(road_number) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000013?line=2'>3</a>\u001b[0m image_data \u001b[39m=\u001b[39m Data(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39m./data\u001b[39m\u001b[39m\"\u001b[39m, filename))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000013?line=3'>4</a>\u001b[0m detected_signs, output_image \u001b[39m=\u001b[39m evaluate_image(image_data)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000013?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./data/annotations/road\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(road_number) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.xml\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f: data \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000013?line=6'>7</a>\u001b[0m root \u001b[39m=\u001b[39m ET\u001b[39m.\u001b[39mfromstring(data)\n",
      "\u001b[1;32mc:\\Users\\diogo\\Documents\\cgra\\feup-vcom-traffic-sign-detection\\guide.ipynb Cell 13'\u001b[0m in \u001b[0;36mevaluate_image\u001b[1;34m(image_data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000011?line=80'>81</a>\u001b[0m \u001b[39mif\u001b[39;00m stop_count \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000011?line=81'>82</a>\u001b[0m     stop_sign \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39m./data\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstop.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000011?line=82'>83</a>\u001b[0m     matching_stop(stop_sign, image_data\u001b[39m.\u001b[39;49mimage)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000011?line=84'>85</a>\u001b[0m \u001b[39mreturn\u001b[39;00m signs, image_data\n",
      "\u001b[1;32mc:\\Users\\diogo\\Documents\\cgra\\feup-vcom-traffic-sign-detection\\guide.ipynb Cell 7'\u001b[0m in \u001b[0;36mmatching_stop\u001b[1;34m(img1, img2)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000018?line=38'>39</a>\u001b[0m     h,w \u001b[39m=\u001b[39m img1\u001b[39m.\u001b[39mshape\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000018?line=39'>40</a>\u001b[0m     pts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfloat32([ [\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m],[\u001b[39m0\u001b[39m,h\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],[w\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,h\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],[w\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m] ])\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000018?line=40'>41</a>\u001b[0m     dst \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mperspectiveTransform(pts,M)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000018?line=42'>43</a>\u001b[0m     img2 \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mpolylines(img2,[np\u001b[39m.\u001b[39mint32(dst)],\u001b[39mTrue\u001b[39;00m,\u001b[39m255\u001b[39m,\u001b[39m3\u001b[39m, cv2\u001b[39m.\u001b[39mLINE_AA)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000018?line=44'>45</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/diogo/Documents/cgra/feup-vcom-traffic-sign-detection/guide.ipynb#ch0000018?line=45'>46</a>\u001b[0m     \u001b[39m#print(\"Not enough matches are found - \" + str(len(good)) + \"/\" + str(MIN_MATCH_COUNT))\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\matmul.dispatch.cpp:550: error: (-215:Assertion failed) scn + 1 == m.cols in function 'cv::perspectiveTransform'\n"
     ]
    }
   ],
   "source": [
    "def process_image(road_number):\n",
    "        filename = 'road' + str(road_number) + '.png'\n",
    "        image_data = Data(os.path.join(\"./data\", filename))\n",
    "        detected_signs, output_image = evaluate_image(image_data)\n",
    "        \n",
    "        with open('./data/annotations/road' + str(road_number) + '.xml', 'r') as f: data = f.read()\n",
    "        root = ET.fromstring(data)\n",
    "\n",
    "        real_signs = []\n",
    "        for content in root.findall('.//object/name'):\n",
    "                real_signs.append(content.text)\n",
    "\n",
    "        print('------')\n",
    "        print(detected_signs)\n",
    "        print(real_signs)\n",
    "\n",
    "        # Show the output image\n",
    "        if OUTPUT == \"WINDOW\":\n",
    "                cv2.imshow('Final Result', output_image.image)\n",
    "        else:\n",
    "                cv2.imwrite(f'output/{filename}', image_data.image)\n",
    "\n",
    "if not os.path.exists('./output'):\n",
    "        os.makedirs('./output')\n",
    "\n",
    "if FILENAME == \"ALL\":\n",
    "        for i in range(53, 876):\n",
    "                process_image(i)\n",
    "                print(\"print\" + str(i))\n",
    "                k = cv2.waitKey(0)   \n",
    "                cv2.destroyAllWindows()\n",
    "                if k == 27:\n",
    "                        break\n",
    "else:\n",
    "        process_image(FILENAME)\n",
    "\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce41d2c9079f2c783da2c5a9243470309928341d0f7f9360b413f846a0d7760d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
